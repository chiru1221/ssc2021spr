{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd0f15a31bade87f703e513ad3072b10c6f06d3ccd96b6d16858f67eed0f22e28ac",
   "display_name": "Python 3.8.5 64-bit ('kaggle': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "import  os\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from utils import Optuna_for_LGB\n",
    "from preprocess import convert_notation, region_encoding, grouping_by_region, group_to_feature, require_median_dict\n",
    "PATH = '../data/'\n",
    "TRAIN = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "dict_keys(['acousticness', 'positiveness', 'danceability', 'energy', 'liveness', 'speechiness', 'instrumentalness', 'popularity_mean', 'popularity_std', 'popularity_kurtosis', 'popularity_skew', 'duration_ms_mean', 'duration_ms_std', 'duration_ms_kurtosis', 'duration_ms_skew', 'acousticness_mean', 'acousticness_std', 'acousticness_kurtosis', 'acousticness_skew', 'positiveness_mean', 'positiveness_std', 'positiveness_kurtosis', 'positiveness_skew', 'danceability_mean', 'danceability_std', 'danceability_kurtosis', 'danceability_skew', 'loudness_mean', 'loudness_std', 'loudness_kurtosis', 'loudness_skew', 'energy_mean', 'energy_std', 'energy_kurtosis', 'energy_skew', 'liveness_mean', 'liveness_std', 'liveness_kurtosis', 'liveness_skew', 'speechiness_mean', 'speechiness_std', 'speechiness_kurtosis', 'speechiness_skew', 'instrumentalness_mean', 'instrumentalness_std', 'instrumentalness_kurtosis', 'instrumentalness_skew', 'tempo_mean', 'tempo_std', 'tempo_kurtosis', 'tempo_skew', 'tempo_min_mean', 'tempo_min_std', 'tempo_min_kurtosis', 'tempo_min_skew', 'tempo_max_mean', 'tempo_max_std', 'tempo_max_kurtosis', 'tempo_max_skew', 'popularity_region_mean_diff', 'popularity_region_mean_diff_region_std_diff', 'popularity_region_mean_div_kurtosis', 'popularity_region_mean_div_skew', 'duration_ms_region_mean_diff', 'duration_ms_region_mean_diff_region_std_diff', 'duration_ms_region_mean_div_kurtosis', 'duration_ms_region_mean_div_skew', 'acousticness_region_mean_diff', 'acousticness_region_mean_diff_region_std_diff', 'acousticness_region_mean_div_kurtosis', 'acousticness_region_mean_div_skew', 'positiveness_region_mean_diff', 'positiveness_region_mean_diff_region_std_diff', 'positiveness_region_mean_div_kurtosis', 'positiveness_region_mean_div_skew', 'danceability_region_mean_diff', 'danceability_region_mean_diff_region_std_diff', 'danceability_region_mean_div_kurtosis', 'danceability_region_mean_div_skew', 'loudness_region_mean_diff', 'loudness_region_mean_diff_region_std_diff', 'loudness_region_mean_div_kurtosis', 'loudness_region_mean_div_skew', 'energy_region_mean_diff', 'energy_region_mean_diff_region_std_diff', 'energy_region_mean_div_kurtosis', 'energy_region_mean_div_skew', 'liveness_region_mean_diff', 'liveness_region_mean_diff_region_std_diff', 'liveness_region_mean_div_kurtosis', 'liveness_region_mean_div_skew', 'speechiness_region_mean_diff', 'speechiness_region_mean_diff_region_std_diff', 'speechiness_region_mean_div_kurtosis', 'speechiness_region_mean_div_skew', 'instrumentalness_region_mean_diff', 'instrumentalness_region_mean_diff_region_std_diff', 'instrumentalness_region_mean_div_kurtosis', 'instrumentalness_region_mean_div_skew', 'tempo_region_mean_diff', 'tempo_region_mean_diff_region_std_diff', 'tempo_region_mean_div_kurtosis', 'tempo_region_mean_div_skew', 'tempo_min_region_mean_diff', 'tempo_min_region_mean_diff_region_std_diff', 'tempo_min_region_mean_div_kurtosis', 'tempo_min_region_mean_div_skew', 'tempo_max_region_mean_diff', 'tempo_max_region_mean_diff_region_std_diff', 'tempo_max_region_mean_div_kurtosis', 'tempo_max_region_mean_div_skew'])\n(4043, 16) (4046, 15)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   index  genre  popularity  duration_ms  acousticness  positiveness  \\\n",
       "0      0     10          11       201094      0.112811      0.157247   \n",
       "1      1      8          69       308493      0.101333      0.346563   \n",
       "2      2      3          43       197225      0.496420      0.265391   \n",
       "3      3     10          45       301092      0.165667      0.245533   \n",
       "4      4      3          57       277348      0.190720      0.777578   \n",
       "\n",
       "   danceability  loudness    energy  liveness  speechiness  instrumentalness  \\\n",
       "0      0.187841 -1.884852  0.893918  0.363568     0.390108          0.888884   \n",
       "1      0.554444 -5.546495  0.874409  0.193892     0.161497          0.123910   \n",
       "2      0.457642 -9.255670  0.439933  0.217146     0.369057          0.166470   \n",
       "3      0.356578 -5.088788  0.868704  0.377025     0.226677          0.175399   \n",
       "4      0.830479 -3.933896  0.650149  0.169323     0.222488          0.226030   \n",
       "\n",
       "   tempo  region  tempo_min  tempo_max  \n",
       "0     31       7        121        152  \n",
       "1     23       8        153        176  \n",
       "2     12       4         64         76  \n",
       "3     15       2        177        192  \n",
       "4     23      19         97        120  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>genre</th>\n      <th>popularity</th>\n      <th>duration_ms</th>\n      <th>acousticness</th>\n      <th>positiveness</th>\n      <th>danceability</th>\n      <th>loudness</th>\n      <th>energy</th>\n      <th>liveness</th>\n      <th>speechiness</th>\n      <th>instrumentalness</th>\n      <th>tempo</th>\n      <th>region</th>\n      <th>tempo_min</th>\n      <th>tempo_max</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>10</td>\n      <td>11</td>\n      <td>201094</td>\n      <td>0.112811</td>\n      <td>0.157247</td>\n      <td>0.187841</td>\n      <td>-1.884852</td>\n      <td>0.893918</td>\n      <td>0.363568</td>\n      <td>0.390108</td>\n      <td>0.888884</td>\n      <td>31</td>\n      <td>7</td>\n      <td>121</td>\n      <td>152</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>8</td>\n      <td>69</td>\n      <td>308493</td>\n      <td>0.101333</td>\n      <td>0.346563</td>\n      <td>0.554444</td>\n      <td>-5.546495</td>\n      <td>0.874409</td>\n      <td>0.193892</td>\n      <td>0.161497</td>\n      <td>0.123910</td>\n      <td>23</td>\n      <td>8</td>\n      <td>153</td>\n      <td>176</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>3</td>\n      <td>43</td>\n      <td>197225</td>\n      <td>0.496420</td>\n      <td>0.265391</td>\n      <td>0.457642</td>\n      <td>-9.255670</td>\n      <td>0.439933</td>\n      <td>0.217146</td>\n      <td>0.369057</td>\n      <td>0.166470</td>\n      <td>12</td>\n      <td>4</td>\n      <td>64</td>\n      <td>76</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>10</td>\n      <td>45</td>\n      <td>301092</td>\n      <td>0.165667</td>\n      <td>0.245533</td>\n      <td>0.356578</td>\n      <td>-5.088788</td>\n      <td>0.868704</td>\n      <td>0.377025</td>\n      <td>0.226677</td>\n      <td>0.175399</td>\n      <td>15</td>\n      <td>2</td>\n      <td>177</td>\n      <td>192</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>3</td>\n      <td>57</td>\n      <td>277348</td>\n      <td>0.190720</td>\n      <td>0.777578</td>\n      <td>0.830479</td>\n      <td>-3.933896</td>\n      <td>0.650149</td>\n      <td>0.169323</td>\n      <td>0.222488</td>\n      <td>0.226030</td>\n      <td>23</td>\n      <td>19</td>\n      <td>97</td>\n      <td>120</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "train_df = pd.read_csv(PATH + 'train.csv')\n",
    "test_df = pd.read_csv(PATH + 'test.csv')\n",
    "train_df = convert_notation(train_df)\n",
    "test_df = convert_notation(test_df)\n",
    "train_df, test_df, le = region_encoding(train_df, test_df)\n",
    "columns = list(test_df.columns)\n",
    "columns.remove('index')\n",
    "columns.remove('region')\n",
    "group = grouping_by_region(train_df, test_df, columns.copy())\n",
    "na_train = group_to_feature(train_df[test_df.columns].copy(), group, columns.copy())\n",
    "na_test = group_to_feature(test_df.copy(), group, columns.copy())\n",
    "median_dict = require_median_dict(na_test, test_df)\n",
    "print(median_dict.keys())\n",
    "print(train_df.shape, test_df.shape)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "objective_args = {\n",
    "    'num_leaves': {\n",
    "        'type': 'int',\n",
    "        'suggest_args': {\n",
    "            'name': 'num_leaves',\n",
    "            'low': 2,\n",
    "            'high': 128,\n",
    "        }\n",
    "    },\n",
    "    'max_depth': {\n",
    "        'type': 'int',\n",
    "        'suggest_args': {\n",
    "            'name': 'max_depth',\n",
    "            'low': 3,\n",
    "            'high': 8,\n",
    "        }\n",
    "    },\n",
    "    'min_data_in_leaf': {\n",
    "        'type': 'int',\n",
    "        'suggest_args': {\n",
    "            'name': 'min_data_in_leaf',\n",
    "            'low': 5,\n",
    "            'high': 90,\n",
    "        }\n",
    "    },\n",
    "    'n_estimators': {\n",
    "        'type': 'int',\n",
    "        'suggest_args': {\n",
    "            'name': 'n_estimators',\n",
    "            'low': 100,\n",
    "            'high': 1000,\n",
    "        }\n",
    "    },\n",
    "    'learning_rate': {\n",
    "        'type': 'uniform',\n",
    "        'suggest_args': {\n",
    "            'name': 'learning_rate',\n",
    "            'low': 0.0001,\n",
    "            'high': 0.1\n",
    "        }\n",
    "    },\n",
    "    'bagging_fraction': {\n",
    "        'type': 'uniform',\n",
    "        'suggest_args': {\n",
    "            'name': 'bagging_fraction',\n",
    "            'low': 0.0001,\n",
    "            'high': 1.0,\n",
    "        }\n",
    "    },\n",
    "    'feature_fraction': {\n",
    "        'type': 'uniform',\n",
    "        'suggest_args': {\n",
    "            'name': 'feature_fraction',\n",
    "            'low': 0.0001,\n",
    "            'high': 1.0,\n",
    "        }\n",
    "    },\n",
    "    'random_state': {\n",
    "        'type': 'default',\n",
    "        'value': 0\n",
    "    },\n",
    "    'objective': {\n",
    "        'type': 'default',\n",
    "        'value': 'cross_entropy'\n",
    "    },\n",
    "    'num_class': {\n",
    "        'type': 'default',\n",
    "        'value': 11\n",
    "    },\n",
    "    # 'class_weight': {\n",
    "    #     'type': 'default',\n",
    "    #     'value': class_weight\n",
    "    # }\n",
    "    \n",
    "}\n",
    "def evaluate_macroF1_lgb(true, pred):  \n",
    "    # this follows the discussion in https://github.com/Microsoft/LightGBM/issues/1483\n",
    "    pred = pred.reshape(len(np.unique(true)), -1).argmax(axis=0)\n",
    "    f1 = f1_score(true, pred, average='macro')\n",
    "    return ('macroF1', f1, True) \n",
    "\n",
    "pipeline_args = {\n",
    "    'fit_attr': 'fit',\n",
    "    'pred_attr': 'predict',\n",
    "    'fit_args': {\n",
    "        # 'X': x_train,\n",
    "        # 'y': y_train,\n",
    "        # 'eval_set': (x_test, y_test),\n",
    "        'eval_metric': evaluate_macroF1_lgb,\n",
    "        'eval_names': ['validation'],\n",
    "        'early_stopping_rounds': 50,\n",
    "        'verbose': -1,\n",
    "        # 'feature_name': columns\n",
    "        'categorical_feature': ['region']\n",
    "    },\n",
    "    # 'pred_args': {'X': x_test},\n",
    "    'metric': lambda true, pred: f1_score(true, pred, average='macro'),\n",
    "    # 'metric_args': {'true': y_test},\n",
    "    'model': lgb.LGBMClassifier,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "columns = list(test_df.columns)\n",
    "columns.remove('index')\n",
    "target_columns = columns.copy()\n",
    "target_columns.remove('region')\n",
    "cv_num = 10\n",
    "kf = StratifiedKFold(n_splits=cv_num, shuffle=True, random_state=0)\n",
    "name = 'lgb_smote'\n",
    "x, y = train_df[columns], train_df.genre\n",
    "ofl = Optuna_for_LGB()\n",
    "optuna.logging.disable_default_handler()\n",
    "score = list()\n",
    "for cv, (train_valid_idx, test_idx) in enumerate(kf.split(np.zeros(train_df.shape[0]), y)):\n",
    "    x_tv, x_test = x.iloc[train_valid_idx], x.iloc[test_idx]\n",
    "    y_tv, y_test = y.iloc[train_valid_idx], y.iloc[test_idx]\n",
    "    x_test = group_to_feature(x_test, group, target_columns)\n",
    "    for col, median in median_dict.items():\n",
    "        x_test[col] = x_test[col].fillna(value=median)\n",
    "\n",
    "    cv_score = list()\n",
    "    for valid_cv, (train_idx, valid_idx) in enumerate(kf.split(np.zeros(x_tv.shape[0]), y_tv)):\n",
    "        x_train, x_valid = x_tv.iloc[train_idx], x_tv.iloc[valid_idx]\n",
    "        y_train, y_valid = y_tv.iloc[train_idx], y_tv.iloc[valid_idx]\n",
    "\n",
    "        x_train = group_to_feature(x_train, group, target_columns)\n",
    "        x_valid = group_to_feature(x_valid, group, target_columns)\n",
    "\n",
    "        x_train_fill = x_train.copy()\n",
    "        for col, median in median_dict.items():\n",
    "            x_train_fill[col] = x_train_fill[col].fillna(value=median)\n",
    "            x_valid[col] = x_valid[col].fillna(value=median)\n",
    "        x_train = x_train.reset_index(drop=True)\n",
    "        y_train = y_train.reset_index(drop=True)\n",
    "        x_train_fill = x_train_fill.reset_index(drop=True)\n",
    "\n",
    "        smote = SMOTE(random_state=0, n_jobs=-1, k_neighbors=3)\n",
    "        ss_col = list(x_train.columns)\n",
    "        ss_col.remove('region')\n",
    "        ss = StandardScaler()\n",
    "        x_train_fill[ss_col] = ss.fit_transform(x_train_fill[ss_col])\n",
    "        x_train_fill = pd.get_dummies(x_train_fill, columns=['region'])\n",
    "        region_col = list(x_train_fill.columns)[-20:]\n",
    "        x_train_fill[region_col] /= 100.0\n",
    "        x_sample, y_sample = smote.fit_resample(x_train_fill.copy(), y_train.copy())\n",
    "        x_sample = pd.DataFrame(x_sample, columns=x_train_fill.columns)\n",
    "        x_sample[ss_col] = ss.inverse_transform(x_sample[ss_col])\n",
    "        region_onehot = x_sample[region_col].copy()\n",
    "        x_sample = x_sample.drop(columns=region_col)\n",
    "        x_sample['region'] = region_onehot.idxmax(1).apply(lambda x: int(x.split('_')[1]))\n",
    "\n",
    "        x_train = x_sample\n",
    "        y_train = y_sample\n",
    "\n",
    "        pipeline_args['fit_args']['X'] = x_train\n",
    "        pipeline_args['fit_args']['y'] = y_train\n",
    "        pipeline_args['fit_args']['eval_set'] = (x_valid, y_valid)\n",
    "        pipeline_args['fit_args']['feature_name'] = list(x_train.columns)\n",
    "        pipeline_args['pred_args'] = {'X': x_valid}\n",
    "        pipeline_args['metric_args'] = {'true': y_valid}\n",
    "        if TRAIN:\n",
    "            params = ofl.parameter_tuning(pipeline_args, objective_args, 1000, -1, 0)\n",
    "\n",
    "            best_iteration = params.pop('best_iteration_')\n",
    "            model = lgb.LGBMClassifier(**params)\n",
    "            model.fit(**pipeline_args['fit_args'])\n",
    "            y_pred = model.predict_proba(x_test, model.best_iteration_)\n",
    "            model.booster_.save_model('../model/{0}_{1}_{2}.txt'.format(name, cv, valid_cv))\n",
    "        else:\n",
    "            model = lgb.Booster(model_file='../model/{0}_{1}_{2}.txt'.format(name, cv, valid_cv))\n",
    "            y_pred = model.predict(x_test).astype(np.float64)\n",
    "\n",
    "        cv_score.append(f1_score(y_test, np.argmax(y_pred, axis=1), average='macro'))\n",
    "    score.append(cv_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "([[0.16904407546148226,\n",
       "   0.17584521587220903,\n",
       "   0.2127246055062425,\n",
       "   0.14342927119110763,\n",
       "   0.1732359166330746,\n",
       "   0.16483859946905433,\n",
       "   0.21402427115709768,\n",
       "   0.16258909200933733,\n",
       "   0.2362115872052928,\n",
       "   0.13800345543486914],\n",
       "  [0.1353611563168828,\n",
       "   0.1684207343116848,\n",
       "   0.14726503578836195,\n",
       "   0.16514598828170685,\n",
       "   0.18452246293325753,\n",
       "   0.16095901662889853,\n",
       "   0.15340209856750198,\n",
       "   0.1478996948069113,\n",
       "   0.14876199270861068,\n",
       "   0.16337420129949035],\n",
       "  [0.14477403419935683,\n",
       "   0.17756648202937772,\n",
       "   0.14966200466200466,\n",
       "   0.18522949826524526,\n",
       "   0.14599423391181912,\n",
       "   0.16180886396316088,\n",
       "   0.1795059229777529,\n",
       "   0.19829476762784629,\n",
       "   0.18563508833809045,\n",
       "   0.17045014699344418],\n",
       "  [0.15422714518214772,\n",
       "   0.17322363645415204,\n",
       "   0.19114016172099343,\n",
       "   0.15157581976611023,\n",
       "   0.1359535793138141,\n",
       "   0.17498555444327368,\n",
       "   0.12730483656936317,\n",
       "   0.18075201058288862,\n",
       "   0.22836115620910205,\n",
       "   0.16828650479239673],\n",
       "  [0.2108435744343198,\n",
       "   0.2074874740165242,\n",
       "   0.19538874036582207,\n",
       "   0.2492491961266665,\n",
       "   0.15271408062701275,\n",
       "   0.19058835360406123,\n",
       "   0.234622963322789,\n",
       "   0.1847254098865462,\n",
       "   0.17968578843363878,\n",
       "   0.18863376182256195],\n",
       "  [0.14350043137607013,\n",
       "   0.12967128128232847,\n",
       "   0.16979643951554063,\n",
       "   0.22465084891325782,\n",
       "   0.1611424049473009,\n",
       "   0.13733427890919794,\n",
       "   0.13659105263026505,\n",
       "   0.24475095411005743,\n",
       "   0.14199449860977742,\n",
       "   0.1594140617701463],\n",
       "  [0.17325856827478614,\n",
       "   0.15642626885464403,\n",
       "   0.1830759643224166,\n",
       "   0.18825411382590718,\n",
       "   0.14448817537072645,\n",
       "   0.17969833497038099,\n",
       "   0.1901804596871256,\n",
       "   0.1870556793435134,\n",
       "   0.20503655862267103,\n",
       "   0.13764359020718883],\n",
       "  [0.12133498549485043,\n",
       "   0.18347520910734189,\n",
       "   0.17525300855444395,\n",
       "   0.2025188459870603,\n",
       "   0.2000809020230844,\n",
       "   0.14120039072152465,\n",
       "   0.1780946572463273,\n",
       "   0.1707600285968218,\n",
       "   0.17639964688811363,\n",
       "   0.18685327915272298],\n",
       "  [0.17152707124792907,\n",
       "   0.16436265607621106,\n",
       "   0.1766745795775727,\n",
       "   0.1462464517675555,\n",
       "   0.14305522425761613,\n",
       "   0.1618270530969204,\n",
       "   0.14637358844772885,\n",
       "   0.15585380652772096,\n",
       "   0.16141005202983616,\n",
       "   0.10872924164465037],\n",
       "  [0.18371674096431476,\n",
       "   0.11975490175095292,\n",
       "   0.22308844994992924,\n",
       "   0.2617264918160197,\n",
       "   0.1902933454367986,\n",
       "   0.21545489221022043,\n",
       "   0.12894130393419934,\n",
       "   0.17628762775954934,\n",
       "   0.14877449632440556,\n",
       "   0.20819570366740178]],\n",
       " array([0.17899461, 0.15751124, 0.1698921 , 0.16858104, 0.19939393,\n",
       "        0.16488463, 0.17451177, 0.1735971 , 0.15360597, 0.1856234 ]),\n",
       " 0.17265957860028486)"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "score, np.mean(score, axis=1), np.mean(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = pd.read_csv(PATH + 'sample_submit.csv', header=None, names=['ID', 'Pred'])\n",
    "prediction = np.zeros((test_df.shape[0], 11), dtype=np.float64)\n",
    "x = test_df[columns]\n",
    "for i in range(cv_num):\n",
    "    for j in range(cv_num):\n",
    "        model = lgb.Booster(model_file='../model/{0}_{1}_{2}.txt'.format(name, i, j))\n",
    "        pred = model.predict(x).astype(np.float64)\n",
    "        prediction += pred\n",
    "prediction /= 100.0\n",
    "sub_df.Pred = np.argmax(prediction, axis=1)\n",
    "sub_df.head()"
   ]
  }
 ]
}