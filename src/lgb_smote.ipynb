{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd0f15a31bade87f703e513ad3072b10c6f06d3ccd96b6d16858f67eed0f22e28ac",
   "display_name": "Python 3.8.5 64-bit ('kaggle': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "import  os\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from utils import Optuna_for_LGB\n",
    "from preprocess import convert_notation, region_encoding, grouping_by_region, group_to_feature, require_median_dict\n",
    "PATH = '../data/'\n",
    "TRAIN = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "dict_keys(['acousticness', 'positiveness', 'danceability', 'energy', 'liveness', 'speechiness', 'instrumentalness', 'popularity_mean', 'popularity_std', 'popularity_kurtosis', 'popularity_skew', 'duration_ms_mean', 'duration_ms_std', 'duration_ms_kurtosis', 'duration_ms_skew', 'acousticness_mean', 'acousticness_std', 'acousticness_kurtosis', 'acousticness_skew', 'positiveness_mean', 'positiveness_std', 'positiveness_kurtosis', 'positiveness_skew', 'danceability_mean', 'danceability_std', 'danceability_kurtosis', 'danceability_skew', 'loudness_mean', 'loudness_std', 'loudness_kurtosis', 'loudness_skew', 'energy_mean', 'energy_std', 'energy_kurtosis', 'energy_skew', 'liveness_mean', 'liveness_std', 'liveness_kurtosis', 'liveness_skew', 'speechiness_mean', 'speechiness_std', 'speechiness_kurtosis', 'speechiness_skew', 'instrumentalness_mean', 'instrumentalness_std', 'instrumentalness_kurtosis', 'instrumentalness_skew', 'tempo_mean', 'tempo_std', 'tempo_kurtosis', 'tempo_skew', 'tempo_min_mean', 'tempo_min_std', 'tempo_min_kurtosis', 'tempo_min_skew', 'tempo_max_mean', 'tempo_max_std', 'tempo_max_kurtosis', 'tempo_max_skew', 'popularity_region_mean_diff', 'popularity_region_mean_diff_region_std_diff', 'popularity_region_mean_div_kurtosis', 'popularity_region_mean_div_skew', 'duration_ms_region_mean_diff', 'duration_ms_region_mean_diff_region_std_diff', 'duration_ms_region_mean_div_kurtosis', 'duration_ms_region_mean_div_skew', 'acousticness_region_mean_diff', 'acousticness_region_mean_diff_region_std_diff', 'acousticness_region_mean_div_kurtosis', 'acousticness_region_mean_div_skew', 'positiveness_region_mean_diff', 'positiveness_region_mean_diff_region_std_diff', 'positiveness_region_mean_div_kurtosis', 'positiveness_region_mean_div_skew', 'danceability_region_mean_diff', 'danceability_region_mean_diff_region_std_diff', 'danceability_region_mean_div_kurtosis', 'danceability_region_mean_div_skew', 'loudness_region_mean_diff', 'loudness_region_mean_diff_region_std_diff', 'loudness_region_mean_div_kurtosis', 'loudness_region_mean_div_skew', 'energy_region_mean_diff', 'energy_region_mean_diff_region_std_diff', 'energy_region_mean_div_kurtosis', 'energy_region_mean_div_skew', 'liveness_region_mean_diff', 'liveness_region_mean_diff_region_std_diff', 'liveness_region_mean_div_kurtosis', 'liveness_region_mean_div_skew', 'speechiness_region_mean_diff', 'speechiness_region_mean_diff_region_std_diff', 'speechiness_region_mean_div_kurtosis', 'speechiness_region_mean_div_skew', 'instrumentalness_region_mean_diff', 'instrumentalness_region_mean_diff_region_std_diff', 'instrumentalness_region_mean_div_kurtosis', 'instrumentalness_region_mean_div_skew', 'tempo_region_mean_diff', 'tempo_region_mean_diff_region_std_diff', 'tempo_region_mean_div_kurtosis', 'tempo_region_mean_div_skew', 'tempo_min_region_mean_diff', 'tempo_min_region_mean_diff_region_std_diff', 'tempo_min_region_mean_div_kurtosis', 'tempo_min_region_mean_div_skew', 'tempo_max_region_mean_diff', 'tempo_max_region_mean_diff_region_std_diff', 'tempo_max_region_mean_div_kurtosis', 'tempo_max_region_mean_div_skew'])\n(4043, 16) (4046, 15)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   index  genre  popularity  duration_ms  acousticness  positiveness  \\\n",
       "0      0     10          11       201094      0.112811      0.157247   \n",
       "1      1      8          69       308493      0.101333      0.346563   \n",
       "2      2      3          43       197225      0.496420      0.265391   \n",
       "3      3     10          45       301092      0.165667      0.245533   \n",
       "4      4      3          57       277348      0.190720      0.777578   \n",
       "\n",
       "   danceability  loudness    energy  liveness  speechiness  instrumentalness  \\\n",
       "0      0.187841 -1.884852  0.893918  0.363568     0.390108          0.888884   \n",
       "1      0.554444 -5.546495  0.874409  0.193892     0.161497          0.123910   \n",
       "2      0.457642 -9.255670  0.439933  0.217146     0.369057          0.166470   \n",
       "3      0.356578 -5.088788  0.868704  0.377025     0.226677          0.175399   \n",
       "4      0.830479 -3.933896  0.650149  0.169323     0.222488          0.226030   \n",
       "\n",
       "   tempo  region  tempo_min  tempo_max  \n",
       "0     31       7        121        152  \n",
       "1     23       8        153        176  \n",
       "2     12       4         64         76  \n",
       "3     15       2        177        192  \n",
       "4     23      19         97        120  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>genre</th>\n      <th>popularity</th>\n      <th>duration_ms</th>\n      <th>acousticness</th>\n      <th>positiveness</th>\n      <th>danceability</th>\n      <th>loudness</th>\n      <th>energy</th>\n      <th>liveness</th>\n      <th>speechiness</th>\n      <th>instrumentalness</th>\n      <th>tempo</th>\n      <th>region</th>\n      <th>tempo_min</th>\n      <th>tempo_max</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>10</td>\n      <td>11</td>\n      <td>201094</td>\n      <td>0.112811</td>\n      <td>0.157247</td>\n      <td>0.187841</td>\n      <td>-1.884852</td>\n      <td>0.893918</td>\n      <td>0.363568</td>\n      <td>0.390108</td>\n      <td>0.888884</td>\n      <td>31</td>\n      <td>7</td>\n      <td>121</td>\n      <td>152</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>8</td>\n      <td>69</td>\n      <td>308493</td>\n      <td>0.101333</td>\n      <td>0.346563</td>\n      <td>0.554444</td>\n      <td>-5.546495</td>\n      <td>0.874409</td>\n      <td>0.193892</td>\n      <td>0.161497</td>\n      <td>0.123910</td>\n      <td>23</td>\n      <td>8</td>\n      <td>153</td>\n      <td>176</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>3</td>\n      <td>43</td>\n      <td>197225</td>\n      <td>0.496420</td>\n      <td>0.265391</td>\n      <td>0.457642</td>\n      <td>-9.255670</td>\n      <td>0.439933</td>\n      <td>0.217146</td>\n      <td>0.369057</td>\n      <td>0.166470</td>\n      <td>12</td>\n      <td>4</td>\n      <td>64</td>\n      <td>76</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>10</td>\n      <td>45</td>\n      <td>301092</td>\n      <td>0.165667</td>\n      <td>0.245533</td>\n      <td>0.356578</td>\n      <td>-5.088788</td>\n      <td>0.868704</td>\n      <td>0.377025</td>\n      <td>0.226677</td>\n      <td>0.175399</td>\n      <td>15</td>\n      <td>2</td>\n      <td>177</td>\n      <td>192</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>3</td>\n      <td>57</td>\n      <td>277348</td>\n      <td>0.190720</td>\n      <td>0.777578</td>\n      <td>0.830479</td>\n      <td>-3.933896</td>\n      <td>0.650149</td>\n      <td>0.169323</td>\n      <td>0.222488</td>\n      <td>0.226030</td>\n      <td>23</td>\n      <td>19</td>\n      <td>97</td>\n      <td>120</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "train_df = pd.read_csv(PATH + 'train.csv')\n",
    "test_df = pd.read_csv(PATH + 'test.csv')\n",
    "train_df = convert_notation(train_df)\n",
    "test_df = convert_notation(test_df)\n",
    "train_df, test_df, le = region_encoding(train_df, test_df)\n",
    "columns = list(test_df.columns)\n",
    "columns.remove('index')\n",
    "columns.remove('region')\n",
    "group = grouping_by_region(train_df, test_df, columns.copy())\n",
    "na_train = group_to_feature(train_df[test_df.columns].copy(), group, columns.copy())\n",
    "na_test = group_to_feature(test_df.copy(), group, columns.copy())\n",
    "median_dict = require_median_dict(na_test, test_df)\n",
    "print(median_dict.keys())\n",
    "print(train_df.shape, test_df.shape)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "objective_args = {\n",
    "    'num_leaves': {\n",
    "        'type': 'int',\n",
    "        'suggest_args': {\n",
    "            'name': 'num_leaves',\n",
    "            'low': 2,\n",
    "            'high': 128,\n",
    "        }\n",
    "    },\n",
    "    'max_depth': {\n",
    "        'type': 'int',\n",
    "        'suggest_args': {\n",
    "            'name': 'max_depth',\n",
    "            'low': 3,\n",
    "            'high': 8,\n",
    "        }\n",
    "    },\n",
    "    'min_data_in_leaf': {\n",
    "        'type': 'int',\n",
    "        'suggest_args': {\n",
    "            'name': 'min_data_in_leaf',\n",
    "            'low': 5,\n",
    "            'high': 90,\n",
    "        }\n",
    "    },\n",
    "    'n_estimators': {\n",
    "        'type': 'int',\n",
    "        'suggest_args': {\n",
    "            'name': 'n_estimators',\n",
    "            'low': 100,\n",
    "            'high': 1000,\n",
    "        }\n",
    "    },\n",
    "    'learning_rate': {\n",
    "        'type': 'uniform',\n",
    "        'suggest_args': {\n",
    "            'name': 'learning_rate',\n",
    "            'low': 0.0001,\n",
    "            'high': 0.1\n",
    "        }\n",
    "    },\n",
    "    'bagging_fraction': {\n",
    "        'type': 'uniform',\n",
    "        'suggest_args': {\n",
    "            'name': 'bagging_fraction',\n",
    "            'low': 0.0001,\n",
    "            'high': 1.0,\n",
    "        }\n",
    "    },\n",
    "    'feature_fraction': {\n",
    "        'type': 'uniform',\n",
    "        'suggest_args': {\n",
    "            'name': 'feature_fraction',\n",
    "            'low': 0.0001,\n",
    "            'high': 1.0,\n",
    "        }\n",
    "    },\n",
    "    'random_state': {\n",
    "        'type': 'default',\n",
    "        'value': 0\n",
    "    },\n",
    "    'objective': {\n",
    "        'type': 'default',\n",
    "        'value': 'cross_entropy'\n",
    "    },\n",
    "    'num_class': {\n",
    "        'type': 'default',\n",
    "        'value': 11\n",
    "    },\n",
    "    # 'class_weight': {\n",
    "    #     'type': 'default',\n",
    "    #     'value': class_weight\n",
    "    # }\n",
    "    \n",
    "}\n",
    "def evaluate_macroF1_lgb(true, pred):  \n",
    "    # this follows the discussion in https://github.com/Microsoft/LightGBM/issues/1483\n",
    "    pred = pred.reshape(len(np.unique(true)), -1).argmax(axis=0)\n",
    "    f1 = f1_score(true, pred, average='macro')\n",
    "    return ('macroF1', f1, True) \n",
    "\n",
    "pipeline_args = {\n",
    "    'fit_attr': 'fit',\n",
    "    'pred_attr': 'predict',\n",
    "    'fit_args': {\n",
    "        # 'X': x_train,\n",
    "        # 'y': y_train,\n",
    "        # 'eval_set': (x_test, y_test),\n",
    "        'eval_metric': evaluate_macroF1_lgb,\n",
    "        'eval_names': ['validation'],\n",
    "        'early_stopping_rounds': 50,\n",
    "        'verbose': -1,\n",
    "        # 'feature_name': columns\n",
    "        'categorical_feature': ['region']\n",
    "    },\n",
    "    # 'pred_args': {'X': x_test},\n",
    "    'metric': lambda true, pred: f1_score(true, pred, average='macro'),\n",
    "    # 'metric_args': {'true': y_test},\n",
    "    'model': lgb.LGBMClassifier,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "columns = list(test_df.columns)\n",
    "columns.remove('index')\n",
    "target_columns = columns.copy()\n",
    "target_columns.remove('region')\n",
    "cv_num = 10\n",
    "kf = StratifiedKFold(n_splits=cv_num, shuffle=True, random_state=0)\n",
    "name = 'lgb_smote'\n",
    "x, y = train_df[columns], train_df.genre\n",
    "ofl = Optuna_for_LGB()\n",
    "optuna.logging.disable_default_handler()\n",
    "score = list()\n",
    "for cv, (train_valid_idx, test_idx) in enumerate(kf.split(np.zeros(train_df.shape[0]), y)):\n",
    "    x_tv, x_test = x.iloc[train_valid_idx], x.iloc[test_idx]\n",
    "    y_tv, y_test = y.iloc[train_valid_idx], y.iloc[test_idx]\n",
    "    x_test = group_to_feature(x_test, group, target_columns)\n",
    "    for col, median in median_dict.items():\n",
    "        x_test[col] = x_test[col].fillna(value=median)\n",
    "\n",
    "    cv_score = list()\n",
    "    for valid_cv, (train_idx, valid_idx) in enumerate(kf.split(np.zeros(x_tv.shape[0]), y_tv)):\n",
    "        x_train, x_valid = x_tv.iloc[train_idx], x_tv.iloc[valid_idx]\n",
    "        y_train, y_valid = y_tv.iloc[train_idx], y_tv.iloc[valid_idx]\n",
    "\n",
    "        x_train = group_to_feature(x_train, group, target_columns)\n",
    "        x_valid = group_to_feature(x_valid, group, target_columns)\n",
    "\n",
    "        x_train_fill = x_train.copy()\n",
    "        for col, median in median_dict.items():\n",
    "            x_train_fill[col] = x_train_fill[col].fillna(value=median)\n",
    "            x_valid[col] = x_valid[col].fillna(value=median)\n",
    "        x_train = x_train.reset_index(drop=True)\n",
    "        y_train = y_train.reset_index(drop=True)\n",
    "        x_train_fill = x_train_fill.reset_index(drop=True)\n",
    "\n",
    "        smote = SMOTE(random_state=0, n_jobs=-1, k_neighbors=5)\n",
    "        ss_col = list(x_train.columns)\n",
    "        ss_col.remove('region')\n",
    "        ss = StandardScaler()\n",
    "        x_train_fill[ss_col] = ss.fit_transform(x_train_fill[ss_col])\n",
    "        x_train_fill = pd.get_dummies(x_train_fill, columns=['region'])\n",
    "        region_col = list(x_train_fill.columns)[-20:]\n",
    "        x_train_fill[region_col] /= 100.0\n",
    "        x_sample, y_sample = smote.fit_resample(x_train_fill.copy(), y_train.copy())\n",
    "        x_sample = pd.DataFrame(x_sample, columns=x_train_fill.columns)\n",
    "        x_sample[ss_col] = ss.inverse_transform(x_sample[ss_col])\n",
    "        region_onehot = x_sample[region_col].copy()\n",
    "        x_sample = x_sample.drop(columns=region_col)\n",
    "        x_sample['region'] = region_onehot.idxmax(1).apply(lambda x: int(x.split('_')[1]))\n",
    "\n",
    "        x_train = x_sample\n",
    "        y_train = y_sample\n",
    "\n",
    "        pipeline_args['fit_args']['X'] = x_train\n",
    "        pipeline_args['fit_args']['y'] = y_train\n",
    "        pipeline_args['fit_args']['eval_set'] = (x_valid, y_valid)\n",
    "        pipeline_args['fit_args']['feature_name'] = list(x_train.columns)\n",
    "        pipeline_args['pred_args'] = {'X': x_valid}\n",
    "        pipeline_args['metric_args'] = {'true': y_valid}\n",
    "        if TRAIN:\n",
    "            params = ofl.parameter_tuning(pipeline_args, objective_args, 1000, -1, 0)\n",
    "\n",
    "            best_iteration = params.pop('best_iteration_')\n",
    "            model = lgb.LGBMClassifier(**params)\n",
    "            model.fit(**pipeline_args['fit_args'])\n",
    "            y_pred = model.predict_proba(x_test, model.best_iteration_)\n",
    "            model.booster_.save_model('../model/{0}_{1}_{2}.txt'.format(name, cv, valid_cv))\n",
    "        else:\n",
    "            model = lgb.Booster(model_file='../model/{0}_{1}_{2}.txt'.format(name, cv, valid_cv))\n",
    "            y_pred = model.predict(x_test).astype(np.float64)\n",
    "\n",
    "        cv_score.append(f1_score(y_test, np.argmax(y_pred, axis=1), average='macro'))\n",
    "    score.append(cv_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "([[0.23178716141032127,\n",
       "   0.24224813674147294,\n",
       "   0.24314262571587336,\n",
       "   0.19968816575989143,\n",
       "   0.23251148522070586,\n",
       "   0.19936690704940171,\n",
       "   0.2851503693737444,\n",
       "   0.2479473854600289,\n",
       "   0.19917251340304382,\n",
       "   0.24212268289101527],\n",
       "  [0.20890733024173194,\n",
       "   0.30044165880004176,\n",
       "   0.24419984062638792,\n",
       "   0.2905669956110037,\n",
       "   0.2334111571263274,\n",
       "   0.20241505281962208,\n",
       "   0.26913780406465343,\n",
       "   0.32695618288978806,\n",
       "   0.2250174586437306,\n",
       "   0.2655598114486518],\n",
       "  [0.22286475413542683,\n",
       "   0.23744992761852626,\n",
       "   0.16968261389018754,\n",
       "   0.19300923680161564,\n",
       "   0.23495733500011576,\n",
       "   0.13331319464234825,\n",
       "   0.24259979234434914,\n",
       "   0.20702900470084507,\n",
       "   0.24360274808418214,\n",
       "   0.19530521843166707],\n",
       "  [0.24278274075510134,\n",
       "   0.1940739697601543,\n",
       "   0.31095671155597204,\n",
       "   0.2069774754609831,\n",
       "   0.2552284169620322,\n",
       "   0.1528876608390085,\n",
       "   0.2582963729557275,\n",
       "   0.20947382720638535,\n",
       "   0.1710589085897107,\n",
       "   0.25961905345082914],\n",
       "  [0.2306482761061555,\n",
       "   0.2523567982055987,\n",
       "   0.27951178130572535,\n",
       "   0.2902416881820385,\n",
       "   0.3151768618392495,\n",
       "   0.29436353508256097,\n",
       "   0.2046578176608964,\n",
       "   0.18588748230907487,\n",
       "   0.2253448395505959,\n",
       "   0.283355012367253],\n",
       "  [0.15697498493624062,\n",
       "   0.21653538267252073,\n",
       "   0.1839940525164332,\n",
       "   0.22055871967612958,\n",
       "   0.22776238948645286,\n",
       "   0.2590312352190981,\n",
       "   0.25397966923345544,\n",
       "   0.15681780902279063,\n",
       "   0.21194542100036481,\n",
       "   0.18397681666450036],\n",
       "  [0.25240126525267087,\n",
       "   0.28188425958645263,\n",
       "   0.2092743672226277,\n",
       "   0.1714660560370946,\n",
       "   0.25721476004038524,\n",
       "   0.2135301275277097,\n",
       "   0.26192404366895433,\n",
       "   0.29797232269303714,\n",
       "   0.21251894365743132,\n",
       "   0.29773718915406017],\n",
       "  [0.16682108120051728,\n",
       "   0.2136926421683064,\n",
       "   0.2942896832420929,\n",
       "   0.23496391000574957,\n",
       "   0.2398462343793413,\n",
       "   0.21599821766424476,\n",
       "   0.16058385786494433,\n",
       "   0.15987541964110896,\n",
       "   0.27433470930078646,\n",
       "   0.24559319442848251],\n",
       "  [0.19433936426715223,\n",
       "   0.20962904317284312,\n",
       "   0.16743300653842505,\n",
       "   0.26821707699392944,\n",
       "   0.1926305688627704,\n",
       "   0.18093138223008357,\n",
       "   0.20751048870520242,\n",
       "   0.16198544068797308,\n",
       "   0.20870145888415018,\n",
       "   0.23420685084496065],\n",
       "  [0.25125733868059236,\n",
       "   0.20312889609263163,\n",
       "   0.27386922021780064,\n",
       "   0.238851283568487,\n",
       "   0.21826321443561858,\n",
       "   0.2309296292344876,\n",
       "   0.2351198674776217,\n",
       "   0.24693277049891685,\n",
       "   0.2498322812538154,\n",
       "   0.27028478767609204]],\n",
       " array([0.23231374, 0.25666133, 0.20798138, 0.22613551, 0.25615441,\n",
       "        0.20715765, 0.24559233, 0.22059989, 0.20255847, 0.24184693]),\n",
       " 0.22970016516579292)"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "score, np.mean(score, axis=1), np.mean(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "LightGBMError",
     "evalue": "The number of features in data (14) is not the same as it was in training data (118).\nYou can set ``predict_disable_shape_check=true`` to discard this error, but please be aware what you are doing.",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLightGBMError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-e84e3ead3634>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv_num\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBooster\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'../model/{0}_{1}_{2}.txt'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[0mprediction\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mprediction\u001b[0m \u001b[1;33m/=\u001b[0m \u001b[1;36m100.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda_rap\\anaconda3\\envs\\kaggle\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, data, start_iteration, num_iteration, raw_score, pred_leaf, pred_contrib, data_has_header, is_reshape, **kwargs)\u001b[0m\n\u001b[0;32m   2955\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2956\u001b[0m                 \u001b[0mnum_iteration\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2957\u001b[1;33m         return predictor.predict(data, start_iteration, num_iteration,\n\u001b[0m\u001b[0;32m   2958\u001b[0m                                  \u001b[0mraw_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred_leaf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred_contrib\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2959\u001b[0m                                  data_has_header, is_reshape)\n",
      "\u001b[1;32m~\\anaconda_rap\\anaconda3\\envs\\kaggle\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, data, start_iteration, num_iteration, raw_score, pred_leaf, pred_contrib, data_has_header, is_reshape)\u001b[0m\n\u001b[0;32m    584\u001b[0m             \u001b[0mpreds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__pred_for_csc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_iteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_iteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredict_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m             \u001b[0mpreds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__pred_for_np2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_iteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_iteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredict_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda_rap\\anaconda3\\envs\\kaggle\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36m__pred_for_np2d\u001b[1;34m(self, mat, start_iteration, num_iteration, predict_type)\u001b[0m\n\u001b[0;32m    675\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mpreds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    676\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 677\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0minner_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_iteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_iteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredict_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    678\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    679\u001b[0m     def __create_sparse_native(self, cs, out_shape, out_ptr_indptr, out_ptr_indices, out_ptr_data,\n",
      "\u001b[1;32m~\\anaconda_rap\\anaconda3\\envs\\kaggle\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36minner_predict\u001b[1;34m(mat, start_iteration, num_iteration, predict_type, preds)\u001b[0m\n\u001b[0;32m    645\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Wrong length of pre-allocated predict array\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    646\u001b[0m             \u001b[0mout_num_preds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_int64\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 647\u001b[1;33m             _safe_call(_LIB.LGBM_BoosterPredictForMat(\n\u001b[0m\u001b[0;32m    648\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    649\u001b[0m                 \u001b[0mptr_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda_rap\\anaconda3\\envs\\kaggle\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36m_safe_call\u001b[1;34m(ret)\u001b[0m\n\u001b[0;32m     53\u001b[0m     \"\"\"\n\u001b[0;32m     54\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mLightGBMError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecode_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLGBM_GetLastError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLightGBMError\u001b[0m: The number of features in data (14) is not the same as it was in training data (118).\nYou can set ``predict_disable_shape_check=true`` to discard this error, but please be aware what you are doing."
     ]
    }
   ],
   "source": [
    "sub_df = pd.read_csv(PATH + 'sample_submit.csv', header=None, names=['ID', 'Pred'])\n",
    "prediction = np.zeros((test_df.shape[0], 11), dtype=np.float64)\n",
    "x = test_df[columns]\n",
    "for i in range(cv_num):\n",
    "    for j in range(cv_num):\n",
    "        model = lgb.Booster(model_file='../model/{0}_{1}_{2}.txt'.format(name, i, j))\n",
    "        pred = model.predict(x).astype(np.float64)\n",
    "        prediction += pred\n",
    "prediction /= 100.0\n",
    "sub_df.Pred = np.argmax(prediction, axis=1)\n",
    "sub_df.head()"
   ]
  }
 ]
}